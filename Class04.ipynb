{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwR/WQv2Q1i4b6sJ8N4VrM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/DataScienceApplicationsInComputerVision/blob/main/Class04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolution in Neural Networks**\n",
        "Convolutional neural networks, or CNNs, are true pillars of computer vision, equipping automated systems to process and understand images and videos in a manner that mirrors human capabilities, but with surprisingly superior accuracy and efficiency. For those delving into this field and wanting to maximize the potential of these networks in practical applications, such as image recognition or video analysis, a thorough understanding of the concept of convolution and its implementation in CNNs is essential."
      ],
      "metadata": {
        "id": "Kp8CE7LILF5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sQERNWTgLCcb"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries:\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CIFAR-10 dataset:\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "NFWWvmdnLaOs",
        "outputId": "a0482213-de25-4405-b8cc-364c639d8b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the input for the interval [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "WBM02_zqMWY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the One-Hot Encoding to the labels and classes:\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "v9Q9IRAkManV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and validation:\n",
        "X_train, X_val = X_train[5000:], X_train[:5000]\n",
        "y_train, y_val = y_train[5000:], y_train[:5000]"
      ],
      "metadata": {
        "id": "1EUkLDVsMcxP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the structure of the CNN:\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D (32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D (64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D (128, (3, 3), activation='relu'),\n",
        "    layers. Flatten(),\n",
        "    layers.Dense (128, activation='relu'),\n",
        "    layers.Dense (10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "Uq0-rdZzLXi7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model:\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "avq52ScuGpND"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model:\n",
        "history = model.fit(X_train, y_train, epochs=15, validation_data=(X_val, y_val), batch_size=64)"
      ],
      "metadata": {
        "id": "WYL3xwfXG1n3",
        "outputId": "98661987-39c8-438a-97f9-eaf5781aab35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 81ms/step - accuracy: 0.0983 - loss: 2.3027 - val_accuracy: 0.0972 - val_loss: 2.3027\n",
            "Epoch 2/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 79ms/step - accuracy: 0.1011 - loss: 2.3027 - val_accuracy: 0.0996 - val_loss: 2.3029\n",
            "Epoch 3/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 79ms/step - accuracy: 0.0977 - loss: 2.3027 - val_accuracy: 0.0972 - val_loss: 2.3029\n",
            "Epoch 4/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 80ms/step - accuracy: 0.0950 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3028\n",
            "Epoch 5/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - accuracy: 0.0991 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3028\n",
            "Epoch 6/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - accuracy: 0.0960 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3029\n",
            "Epoch 7/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 95ms/step - accuracy: 0.0996 - loss: 2.3026 - val_accuracy: 0.0920 - val_loss: 2.3028\n",
            "Epoch 8/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 96ms/step - accuracy: 0.0983 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3027\n",
            "Epoch 9/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 92ms/step - accuracy: 0.0981 - loss: 2.3026 - val_accuracy: 0.1038 - val_loss: 2.3027\n",
            "Epoch 10/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 89ms/step - accuracy: 0.0965 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3029\n",
            "Epoch 11/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - accuracy: 0.0985 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3028\n",
            "Epoch 12/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 94ms/step - accuracy: 0.0991 - loss: 2.3027 - val_accuracy: 0.0920 - val_loss: 2.3027\n",
            "Epoch 13/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 89ms/step - accuracy: 0.0997 - loss: 2.3027 - val_accuracy: 0.0972 - val_loss: 2.3028\n",
            "Epoch 14/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - accuracy: 0.0984 - loss: 2.3026 - val_accuracy: 0.0996 - val_loss: 2.3028\n",
            "Epoch 15/15\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - accuracy: 0.0987 - loss: 2.3027 - val_accuracy: 0.1040 - val_loss: 2.3027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on test data:\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "5XlqsGZ4G5-4",
        "outputId": "948e445b-6333-47f3-f1c1-d8d0465ad4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.1027 - loss: 5.3099\n",
            "Test accuracy: 0.10000000149011612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "Convolutional neural networks (CNNs) are fundamental in the field of computational vision, transforming the way we process and interpret complex images. They can identify patterns ranging from simple borders to more complex shapes and textures. CNNs' utility extends beyond simple visual data processing, transforming our interactions with the visual world. They are essential in industrial automation and autonomous navigation systems, demonstrating a versatility that few technologies have. CNNs' ability to generalize training data and apply them to new situations makes them powerful and reliable in any system requiring precise image and video interpretation."
      ],
      "metadata": {
        "id": "Plv8f4FrH249"
      }
    }
  ]
}